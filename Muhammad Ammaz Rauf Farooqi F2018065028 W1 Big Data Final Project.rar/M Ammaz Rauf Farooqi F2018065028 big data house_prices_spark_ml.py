# -*- coding: utf-8 -*-
"""House prices Spark ML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nF7tmphktl7u9VvBx_53nWEdHKTmtn6a

# Predicting House Prices with Apache Spark
"""

!pip install pyspark

import os
import pandas as pd
import numpy as np

from pyspark import SparkConf, SparkContext
from pyspark.sql import SparkSession, SQLContext

from pyspark.sql.types import *
import pyspark.sql.functions as F
from pyspark.sql.functions import udf, col

from pyspark.ml.regression import LinearRegression
from pyspark.mllib.evaluation import RegressionMetrics

from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel
from pyspark.ml.feature import VectorAssembler, StandardScaler
from pyspark.ml.evaluation import RegressionEvaluator

import seaborn as sns
import matplotlib.pyplot as plt

# Commented out IPython magic to ensure Python compatibility.
# Visualization
from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"

pd.set_option('display.max_columns', 200)
pd.set_option('display.max_colwidth', 400)

from matplotlib import rcParams
sns.set(context='notebook', style='whitegrid', rc={'figure.figsize': (18,4)})
rcParams['figure.figsize'] = 18,4

# %matplotlib inline
# %config InlineBackend.figure_format = 'retina'

# setting random seed for notebook reproducability
rnd_seed=23
np.random.seed=rnd_seed
np.random.set_state=rnd_seed

"""## 2. Creating the Spark Session"""

spark = SparkSession.builder.master("local[2]").appName("Linear-Regression-California-Housing").getOrCreate()

spark

sc = spark.sparkContext
sc

sqlContext = SQLContext(spark.sparkContext)
sqlContext

"""## 3. Load The Data From a File Into a Dataframe"""

HOUSING_DATA = '../input/cal_housing.data'

"""Specifying the schema when loading data into a DataFrame will give better performance than schema inference."""

# define the schema, corresponding to a line in the csv data file.
schema = StructType([
    StructField("long", FloatType(), nullable=True),
    StructField("lat", FloatType(), nullable=True),
    StructField("medage", FloatType(), nullable=True),
    StructField("totrooms", FloatType(), nullable=True),
    StructField("totbdrms", FloatType(), nullable=True),
    StructField("pop", FloatType(), nullable=True),
    StructField("houshlds", FloatType(), nullable=True),
    StructField("medinc", FloatType(), nullable=True),
    StructField("medhv", FloatType(), nullable=True)]
)

# Load housing data
housing_df = spark.read.csv(path=HOUSING_DATA, schema=schema).cache()

# Inspect first five rows
housing_df.take(5)

# Show first five rows
housing_df.show(5)

# show the dataframe columns
housing_df.columns

# show the schema of the dataframe
housing_df.printSchema()

"""## 4. Data Exploration"""

# run a sample selection
housing_df.select('pop','totbdrms').show(10)

"""### 4.1 Distribution of the median age of the people living in the area:"""

# group by housingmedianage and see the distribution
result_df = housing_df.groupBy("medage").count().sort("medage", ascending=False)

result_df.show(10)

result_df.toPandas().plot.bar(x='medage',figsize=(14, 6))

"""Most of the residents are either in their youth or they settle here during their senior years. Some data are showing median age < 10 which seems to be out of place.

### 4.2 Summary Statistics:

Spark DataFrames include some built-in functions for statistical processing. The describe() function performs summary statistics calculations on all numeric columns and returns them as a DataFrame.
"""

(housing_df.describe().select(
                    "summary",
                    F.round("medage", 4).alias("medage"),
                    F.round("totrooms", 4).alias("totrooms"),
                    F.round("totbdrms", 4).alias("totbdrms"),
                    F.round("pop", 4).alias("pop"),
                    F.round("houshlds", 4).alias("houshlds"),
                    F.round("medinc", 4).alias("medinc"),
                    F.round("medhv", 4).alias("medhv"))
                    .show())

"""## 5. Data Preprocessing

### 5.1 Preprocessing The Target Values
First, let's start with the `medianHouseValue`, our dependent variable. To facilitate our working with the target values, we will express the house values in units of 100,000. That means that a target such as `452600.000000` should become `4.526`:
"""

# Adjust the values of `medianHouseValue`
housing_df = housing_df.withColumn("medhv", col("medhv")/100000)

# Show the first 2 lines of `df`
housing_df.show(2)

"""## 6. Feature Engineering

"""

housing_df.columns

# Add the new columns to `df`
housing_df = (housing_df.withColumn("rmsperhh", F.round(col("totrooms")/col("houshlds"), 2))
                       .withColumn("popperhh", F.round(col("pop")/col("houshlds"), 2))
                       .withColumn("bdrmsperrm", F.round(col("totbdrms")/col("totrooms"), 2)))

# Inspect the result
housing_df.show(5)

# Re-order and select columns
housing_df = housing_df.select("medhv", 
                              "totbdrms", 
                              "pop", 
                              "houshlds", 
                              "medinc", 
                              "rmsperhh", 
                              "popperhh", 
                              "bdrmsperrm")

"""### 6.1 Feature Extraction


"""

featureCols = ["totbdrms", "pop", "houshlds", "medinc", "rmsperhh", "popperhh", "bdrmsperrm"]

"""**Use a VectorAssembler to put features into a feature vector column:**"""

# put features into a feature vector column
assembler = VectorAssembler(inputCols=featureCols, outputCol="features")

assembled_df = assembler.transform(housing_df)

assembled_df.show(10, truncate=False)

"""### 6.2 Standardization


"""

# Initialize the `standardScaler`
standardScaler = StandardScaler(inputCol="features", outputCol="features_scaled")

# Fit the DataFrame to the scaler
scaled_df = standardScaler.fit(assembled_df).transform(assembled_df)

# Inspect the result
scaled_df.select("features", "features_scaled").show(10, truncate=False)

"""### 7. Building A Machine Learning Model With Spark ML


"""

# Split the data into train and test sets
train_data, test_data = scaled_df.randomSplit([.8,.2], seed=rnd_seed)

train_data.columns

# Initialize `lr`
lr = (LinearRegression(featuresCol='features_scaled', labelCol="medhv", predictionCol='predmedhv', 
                               maxIter=10, regParam=0.3, elasticNetParam=0.8, standardization=False))

# Fit the data to the model
linearModel = lr.fit(train_data)

"""## 8. Evaluating the Model

### 8.1 Inspect the Model Co-efficients
"""

# Coefficients for the model
linearModel.coefficients

featureCols

# Intercept for the model
linearModel.intercept

coeff_df = pd.DataFrame({"Feature": ["Intercept"] + featureCols, "Co-efficients": np.insert(linearModel.coefficients.toArray(), 0, linearModel.intercept)})
coeff_df = coeff_df[["Feature", "Co-efficients"]]

coeff_df

"""### 8.2 Generating Predictions"""

# Generate predictions
predictions = linearModel.transform(test_data)

# Extract the predictions and the "known" correct labels
predandlabels = predictions.select("predmedhv", "medhv")

predandlabels.show()

"""### 8.3 Inspect the Metrics


"""

# Get the RMSE
print("RMSE: {0}".format(linearModel.summary.rootMeanSquaredError))

print("MAE: {0}".format(linearModel.summary.meanAbsoluteError))

# Get the R2
print("R2: {0}".format(linearModel.summary.r2))

"""**Using the RegressionEvaluator from pyspark.ml package:**"""

evaluator = RegressionEvaluator(predictionCol="predmedhv", labelCol='medhv', metricName='rmse')
print("RMSE: {0}".format(evaluator.evaluate(predandlabels)))

evaluator = RegressionEvaluator(predictionCol="predmedhv", labelCol='medhv', metricName='mae')
print("MAE: {0}".format(evaluator.evaluate(predandlabels)))

evaluator = RegressionEvaluator(predictionCol="predmedhv", labelCol='medhv', metricName='r2')
print("R2: {0}".format(evaluator.evaluate(predandlabels)))

"""**Using the RegressionMetrics from pyspark.mllib package:**"""

# mllib is old so the methods are available in rdd
metrics = RegressionMetrics(predandlabels.rdd)

print("RMSE: {0}".format(metrics.rootMeanSquaredError))

print("MAE: {0}".format(metrics.meanAbsoluteError))

print("R2: {0}".format(metrics.r2))

spark.stop()